import sys
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


residual_loss_weight = 1.0
boundary_loss_weight = 1.0
initial_loss_weight = 1.0
# Define parameters for early stopping and learning rate scheduler
patience = 50
min_delta = 1e-5
learning_rate_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)
#add L2 regularization with a factor of 1e-4
l2_regularizer = tf.keras.regularizers.l2(1e-4)
# Define a custom callback for early stopping
class EarlyStoppingCallback(tf.keras.callbacks.Callback):
    def __init__(self, patience, min_delta):
        super(EarlyStoppingCallback, self).__init__()
        self.patience = patience
        self.min_delta = min_delta
        self.wait = 0
        self.best_loss = float('inf')
        self.stopped_epoch = 0

    def on_epoch_end(self, epoch, logs=None):
        current_loss = logs.get("loss")
        if current_loss is None:
            return

        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                print(f"Early stopping triggered at epoch: {epoch}")

# Define neural network architecture with batch normalization
class PINN(tf.keras.Model):
    def __init__(self):
        super(PINN, self).__init__()
        self.dense1 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.dropout1 = tf.keras.layers.Dropout(0.1)
        self.dense2 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.dropout2 = tf.keras.layers.Dropout(0.1)
        self.dense3 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn3 = tf.keras.layers.BatchNormalization()
        self.dropout3 = tf.keras.layers.Dropout(0.1)
        self.dense4 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn4 = tf.keras.layers.BatchNormalization()
        self.dropout4 = tf.keras.layers.Dropout(0.1)
        self.dense5 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn5 = tf.keras.layers.BatchNormalization()
        self.dropout5 = tf.keras.layers.Dropout(0.1)
        self.dense6 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn6 = tf.keras.layers.BatchNormalization()
        self.dropout6 = tf.keras.layers.Dropout(0.1)
        self.dense7 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn7 = tf.keras.layers.BatchNormalization()
        self.dropout7 = tf.keras.layers.Dropout(0.1)
        self.dense8 = tf.keras.layers.Dense(80, activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2_regularizer)
        self.bn8 = tf.keras.layers.BatchNormalization()
        self.dropout8 = tf.keras.layers.Dropout(0.1)
        self.dense9 = tf.keras.layers.Dense(1, kernel_regularizer=l2_regularizer)

    def call(self, inputs):
        x, t = inputs[:, 0:1], inputs[:, 1:2]
        z = tf.concat([x, t], axis=1)
        z = self.dense1(z)
        z = self.bn1(z)
        z = self.dropout1(z)
        z = self.dense2(z)
        z = self.bn2(z)
        z = self.dropout2(z)
        z = self.dense3(z)
        z = self.bn3(z)
        z = self.dropout3(z)
        z = self.dense4(z)
        z = self.bn4(z)
        z = self.dropout4(z)
        z = self.dense5(z)
        z = self.bn5(z)
        z = self.dropout5(z)
        z = self.dense6(z)
        z = self.bn6(z)
        z = self.dropout6(z)
        z = self.dense7(z)
        z = self.bn7(z)
        z = self.dropout7(z)
        z = self.dense8(z)
        z = self.bn8(z)
        z = self.dropout8(z)
        V = self.dense9(z)
        return V

def pde_residual(model, x_t, L, C, R, G):
    x_t = tf.stop_gradient(x_t)

    with tf.GradientTape(persistent=True) as outer_tape:
        outer_tape.watch(x_t)
        with tf.GradientTape(persistent=True) as inner_tape:
            inner_tape.watch(x_t)
            V = model(x_t)

        dV_dxt = inner_tape.gradient(V, x_t)

    d2V_dx2t2 = outer_tape.gradient(dV_dxt, x_t)

    del inner_tape
    del outer_tape

    dV_dt, dV_dx = dV_dxt[..., 1], dV_dxt[..., 0]
    d2V_dt2, d2V_dx2 = d2V_dx2t2[..., 1], d2V_dx2t2[..., 0]

    residual = (L * C) * d2V_dt2 + (R * C + G * L) * dV_dt + (R * G) * V - d2V_dx2
    return residual

# Define loss function
def loss_function(model, x_t, L, C, R, G, boundary_inputs, boundary_condition, initial_inputs, initial_condition):
    residual = pde_residual(model, x_t, L, C, R, G)
    boundary_loss = tf.reduce_mean(tf.square(boundary_condition - model(boundary_inputs)))
    initial_loss = tf.reduce_mean(tf.square(initial_condition - model(initial_inputs)))
    reg_loss = tf.add_n(model.losses)
    return (residual_loss_weight * tf.reduce_mean(tf.square(residual)) + boundary_loss_weight * boundary_loss + initial_loss_weight * initial_loss + reg_loss)  # Include regularization loss in the total loss

# Define parameters, domain, and boundary conditions
R = 0.1  # ohm per kilometer
L = 2e-3  # henry per kilometer (2 millihenry per kilometer)
G = 10e-6  # siemens per kilometer (10 microsiemens per kilometer)
C = 100e-9  # farad per kilometer (100 nanofarads per kilometer)

x_min, x_max, t_min, t_max = 0.0, 10.0, 0.0, 10.0
n_initial, n_boundary, n_collocation = 100, 100, 10000

# Generate initial condition data
x_initial = tf.constant(np.linspace(x_min, x_max, n_initial)[:, np.newaxis], dtype=tf.float32)
t_initial = tf.constant(np.zeros_like(x_initial), dtype=tf.float32)
initial_inputs = tf.concat([x_initial, t_initial], axis=1)
initial_condition = tf.constant(np.zeros_like(x_initial), dtype=tf.float32)

# Generate boundary condition data
t_boundary = tf.constant(np.linspace(t_min, t_max, n_boundary)[:, np.newaxis], dtype=tf.float32)
x_boundary_1 = tf.constant(np.zeros_like(t_boundary), dtype=tf.float32)
x_boundary_2 = tf.constant(np.ones_like(t_boundary) * x_max, dtype=tf.float32)
boundary_condition_1 = 1200 * tf.sin(120 * np.pi * t_boundary)  # u(x=0, t) = 1200 * sin(120*pi*t)
boundary_condition_2 = 120 * tf.sin(120 * np.pi * t_boundary + 30)  # u(x=10, t) = 120 * sin(120*pi*t + 30)
boundary_inputs = tf.constant(np.vstack([np.hstack([x_boundary_1, t_boundary]), np.hstack([x_boundary_2, t_boundary])]), dtype=tf.float32)
boundary_condition = tf.constant(np.vstack([boundary_condition_1, boundary_condition_2]), dtype=tf.float32)

#Generate collocation points
x_collocation = tf.constant(np.linspace(x_min, x_max, n_collocation)[:, np.newaxis], dtype=tf.float32)
t_collocation = tf.constant(np.linspace(t_min, t_max, n_collocation)[:, np.newaxis], dtype=tf.float32)
x_t_collocation = tf.concat([x_collocation, t_collocation], axis=1)
# Train the model with a learning rate scheduler and early stopping
def train(model, optimizer, x_t_collocation, L, C, R, G, boundary_inputs, boundary_condition, initial_inputs, initial_condition, epochs=100, batch_size=64):
    early_stopping = EarlyStoppingCallback(patience=patience, min_delta=min_delta)

    model.compile(optimizer=optimizer, loss=lambda y_true, y_pred: loss_function(model, x_t_collocation, L, C, R, G, boundary_inputs, boundary_condition, initial_inputs, initial_condition))

    X_train = x_t_collocation
    y_train = np.zeros((x_t_collocation.shape[0], 1))

    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor="loss", factor=0.5, patience=10, verbose=1)
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, reduce_lr], verbose=1)

    # Plot the epoch versus loss
    plt.plot(history.history['loss'])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Epoch vs Loss')
    plt.show()

#Initialize model and optimizer
model = PINN()
optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate_scheduler)

#Train the model
train(model, optimizer, x_t_collocation, L, C, R, G, boundary_inputs, boundary_condition, initial_inputs, initial_condition, epochs=200)

#Evaluate and visualize the results
x_values = np.linspace(x_min, x_max, 100)
t_values = np.linspace(t_min, t_max, 100)
X, T = np.meshgrid(x_values, t_values)

inputs = tf.constant(np.vstack([X.ravel(), T.ravel()]).T, dtype=tf.float32)
#inputs = np.vstack([X.ravel(), T.ravel()]).T
V_values = model(inputs).numpy().reshape(100, 100)

fig, ax = plt.subplots()
c = ax.contourf(X, T, V_values)
fig.colorbar(c, ax=ax)
ax.set_xlabel("x")
ax.set_ylabel("t")
plt.show()

fig = plt.figure()
ax = fig.add_subplot(111, projection="3d")
X, T = np.meshgrid(x_values, t_values)

surf = ax.plot_surface(X, T, V_values, cmap="viridis")
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)

ax.set_xlabel("x")
ax.set_ylabel("t")
ax.set_zlabel("Voltage")

plt.show()

#Voltage versus x
plt.plot(X, V_values)
plt.xlabel('x')
plt.ylabel('Voltage (V)')
plt.title('Voltage versus x')
plt.show()

#Voltage versus t
plt.plot(T, V_values)
plt.xlabel('t')
plt.ylabel('Voltage (V)')
plt.title('Voltage versus t')
plt.show()

def calculate_current(model, inputs, C, G):
    with tf.GradientTape() as tape:
        tape.watch(inputs)
        V = model(inputs)
    dV_dt = tape.gradient(V, inputs)[:, 1:2]

    I = C * dV_dt - G * V
    return I

# Calculate current and power loss
I_values = calculate_current(model, inputs, C, G)
I_values = I_values.numpy().reshape(100, 100)
P_loss = R * I_values**2

# Calculate efficiency
P_input = R * I_values[:, 0]**2
P_total_loss = np.sum(P_loss, axis=1) * (x_max - x_min) / 100
efficiency = (P_input - P_total_loss) / P_input
print(f"System efficiency: {efficiency}")


# Plot power loss
fig, ax = plt.subplots()
c = ax.contourf(X, T, P_loss)
fig.colorbar(c, ax=ax)
ax.set_xlabel("x")
ax.set_ylabel("t")
plt.title("Power Loss")
plt.show()


# Plot power loss versus x
plt.plot(x_values, P_loss.T)
plt.xlabel("x")
plt.ylabel("Power Loss (W/km)")
plt.title("Power Loss versus x")
plt.show()

# Plot heatmap of power loss distribution
fig, ax = plt.subplots()
c = ax.contourf(X, T, P_loss, cmap="viridis")
fig.colorbar(c, ax=ax)
ax.set_xlabel("x")
ax.set_ylabel("t")
plt.title("Power Loss Distribution")
plt.show()

import matplotlib.animation as animation
from IPython.display import HTML

def update_frame(frame):
    t_value = t_values[frame]
    inputs = np.vstack([X.ravel(), np.full_like(X.ravel(), t_value)]).T
    V_frame = model(inputs).numpy().reshape(100, 100)

    line.set_data(X, V_frame)
    return line,

fig, ax = plt.subplots()
line, = ax.plot(X[0], V_values[:, 0])

ax.set_xlim(x_min, x_max)
ax.set_ylim(np.min(V_values), np.max(V_values))
ax.set_xlabel("x")
ax.set_ylabel("Voltage (V)")
ax.set_title("Voltage versus x")

ani = animation.FuncAnimation(fig, update_frame, frames=len(t_values), interval=100, blit=True)
plt.close(fig)

# Display the animation
HTML(ani.to_jshtml())
